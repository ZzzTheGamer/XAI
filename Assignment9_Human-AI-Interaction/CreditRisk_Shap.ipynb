{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ZzzTheGamer/XAI/blob/Assignment9_Human-AI-Interaction/CreditRisk_Shap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kN1ZIVBFYDXc"
   },
   "source": [
    "## Credit Risk Prediction with SHAP Interpretability\n",
    "* External source from https://github.com/AIPI-590-XAI/Duke-AI-XAI/blob/main/explainable-ml-example-notebooks/local_explanations.ipynb\n",
    "* GPT 4o used for polishing the language used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vniQt5lUYF19"
   },
   "source": [
    "### 1. Data Source\n",
    "The dataset utilized in this analysis originates from Kaggle's publicly available dataset titled Credit Risk Dataset (https://www.kaggle.com/datasets/laotse/credit-risk-dataset/data). This dataset contains detailed financial and demographic information to assess and predict the credit risk of individual borrowers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzlsqd_iYURT"
   },
   "source": [
    "### 2. Data Description\n",
    "\n",
    "| Feature Name                | Description                             |\n",
    "|-----------------------------|-----------------------------------------|\n",
    "| person_age               | Age of the borrower                     |\n",
    "| person_income             | Annual income of the borrower           |\n",
    "| person_home_ownership     | Home ownership status                   |\n",
    "| person_emp_length         | Employment length in years              |\n",
    "| loan_intent               | Purpose of the loan                     |\n",
    "| loan_grade                | Risk grade of the loan                  |\n",
    "| loan_amnt                 | Loan amount requested                   |\n",
    "| loan_int_rate             | Loan interest rate                      |\n",
    "| loan_status               | Loan status (0 = non-default, 1 = default) |\n",
    "| loan_percent_income       | Loan amount as percentage of income     |\n",
    "| cb_person_default_on_file | Historical default status               |\n",
    "| cb_preson_cred_hist_length| Length of credit history                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cy15FWf-WA0y"
   },
   "source": [
    "### 3. Importing libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data processing, modeling, and interpretation\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('credit_risk_dataset.csv')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qq5UGUrPvoXF"
   },
   "source": [
    "### 4. Perform label encoding for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Create a dictionary to store mappings of original categorical labels\n",
    "encoding_map = {}\n",
    "\n",
    "# Encode categorical columns\n",
    "for col in data.columns:\n",
    "    if data[col].dtype == 'object':\n",
    "        data[col] = label_encoder.fit_transform(data[col].astype(str))\n",
    "        encoding_map[col] = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "# Print the encoding map\n",
    "print(\"Encoding Map:\")\n",
    "for col, mapping in encoding_map.items():\n",
    "    print(col, \":\", mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4ZX9VGZWMtm"
   },
   "source": [
    "### 5. Splitting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target variable\n",
    "X = data.drop('loan_status', axis=1)\n",
    "y = data['loan_status']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upaFKDR2W3Ss"
   },
   "source": [
    "### 6. Model training and performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to evaluate their performance\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grids for hyperparameter tuning for each model\n",
    "param_grid = {\n",
    "    'Decision Tree': {'max_depth': [3, 5, 7, None]},\n",
    "    'Random Forest': {'n_estimators': [50, 100], 'max_depth': [3, 5, 7]},\n",
    "    'XGBoost': {'n_estimators': [50, 100], 'max_depth': [3, 5, 7]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store the best model and its accuracy\n",
    "best_model, best_score, best_name = None, 0, ''\n",
    "\n",
    "# Perform grid search to find optimal hyperparameters and the corresponding model\n",
    "for name, model in models.items():\n",
    "    grid = GridSearchCV(model, param_grid[name], cv=3, scoring='accuracy')\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f\"{name} best params: {grid.best_params_}\")\n",
    "    y_pred = grid.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {acc}\\n\")\n",
    "    # Update best model if current model performs better\n",
    "    if acc > best_score:\n",
    "        best_score, best_model, best_name = acc, grid.best_estimator_, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the best model and its accuracy\n",
    "print(f\"Best Model: {best_name} with accuracy {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model on the training data\n",
    "best_model = XGBClassifier(max_depth=5, n_estimators=100, random_state=42, eval_metric='logloss')\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G1Del7DZe5S"
   },
   "source": [
    "### 7. Rationale for selecting SHAP\n",
    "\n",
    "SHAP (Shapley Additive Explanations) is an explainability method derived from game theory. It decomposes the prediction of a complex model into additive feature contributions called **Shapley values**. These values represent how each individual feature contributes to the prediction relative to a baseline.\n",
    "\n",
    "**Why we choose SHAP?**\n",
    "\n",
    "- **Global Interpretability**:  \n",
    "  SHAP provides an overview of feature importance across the entire dataset. For instance, it helps identify whether features such as income or loan interest rate generally contribute positively or negatively towards loan default prediction.\n",
    "\n",
    "- **Local Interpretability**:  \n",
    "  SHAP enables explanation at the individual prediction level, clearly indicating why a particular borrower is classified as high or low risk.\n",
    "\n",
    "- **Consistency**:  \n",
    "  Unlike some interpretability methods, SHAP follows consistency principles, ensuring that as a feature's predictive strength increases, its Shapley value magnitude increases proportionally.\n",
    "\n",
    "- **Flexibility**:  \n",
    "  SHAP is flexible and applicable across various model types, including decision trees, neural networks, and linear models, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35c6xz9RXEiP"
   },
   "source": [
    "### 8. Model Interpretation using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SHAP to explain the predictions made by the best model\n",
    "explainer = shap.Explainer(best_model.predict, X_test)\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_FsLHiKxCsf"
   },
   "source": [
    "### 9. Visualize SHAP values to interpret the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsmuviBifNO9"
   },
   "source": [
    "* y-axis interpretation:\n",
    "Features are listed vertically according to their overall importance in predicting loan defaults. The most important features appear at the top (e.g., loan_grade, loan_percent_income).\n",
    "* x-axis interpretation:\n",
    "Positive SHAP values (right side) indicates a higher probability of loan default. Negative SHAP values indicates that the feature value reduces the probability of default.\n",
    "* color interpretation:\n",
    "Red indicates high values for that feature. Blue indicates low values for that feature.\n",
    "- From this graph, we can see for loan_grade, high values (lower-quality loan grades) significantly increase default risk (positive SHAP values to the right).\n",
    "- For loan_percent_income, high loan percentages relative to income (red) significantly increase default risk.\n",
    "- For person_income, higher incomes (red points) reduce default risk (negative SHAP values to the left side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waterfall plot\n",
    "shap.waterfall_plot(shap_values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2TvcM6ZhXN7"
   },
   "source": [
    "* Base value (E[f(x)]): This is the average prediction across all instances in the dataset without additional information (0.13 in this case). It serves as a reference point for interpretation.\n",
    "* SHAP values here represents by arrows showing how each feature pushes the prediction away from the base value.\n",
    "* From this graph, we can see for a borrower, with a contribution of +0.58, loan percent income significantly increases the default probability.\n",
    "* Person home ownership contributes +0.26 to risk, suggesting that this borrower's specific home ownership status (3) significantly increases their default probability.\n",
    "* Loan amount contributes positively (+0.07) but less significantly compared to loan_percent_income.\n",
    "* Factors such as lower loan_grade and others slightly lower the risk (negative values shown in blue, e.g., -0.02) but their contribution is minor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "498Wb55wkQLp"
   },
   "source": [
    "* This graph shows the average absolute contribution (mean SHAP value) of each feature to the model's predictions across the entire dataset.\n",
    "* Here we seletc some top influential features to interpret:\n",
    "* We can see loan_grade (0.10) is most influential feature, indicating that loan grades strongly affect default predictions. Lower quality loan (E,F,G,etc) significantly increase default likelihood.\n",
    "* For loan_percent_income (0.08), we can see loans that represent a high percentage of a borrower's income greatly impact default predictions. Higher loan-to-income ratios generally increase the risk of default.\n",
    "* Person_home_ownership (0.07) also strongly influences risk predictions, indicating different ownership types (such as renting versus owning) have different associated risks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
