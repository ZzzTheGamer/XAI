{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ZzzTheGamer/XAI/blob/Assignment6_Mechanistic_Interpretability/Mechanistic_Interpretability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jof8SmFIvf_C"
   },
   "source": [
    "# Code Report: Decomposing Transformer MLP Activations via a Sparse Autoencoder\n",
    "* Here I use external resources from ChatGPT and https://github.com/shehper/sparse-dictionary-learning/tree/main\n",
    "\n",
    "## 1. Intriduction\n",
    "\n",
    "**Key Finding:**  \n",
    "This report implements a key technique from \"Towards Monosemanticity: Decomposing Language Models With Dictionary Learning\" by using a sparse autoencoder to decompose a Transformer's MLP activations into a large number of interpretable, monosemantic features.\n",
    "\n",
    "**Why Choosing This Technique (Sparse Autoencoder)?**  \n",
    "Transformers tend to store multiple features in superposition, meaning individual neurons are polysemantic. By using a sparse autoencoder for dictionary learning, we can recover a set of features where each one corresponds to a single semantic concept. This is a fundamental insight for mechanistic interpretability.\n",
    "\n",
    "**Plan for Implementation:**  \n",
    "1. Set up configuration parameters and initialize the model.\n",
    "2. Implement the sparse autoencoder (core method).\n",
    "3. Experiment 1: train the autoencoder on synthetic data that simulates MLP activations.\n",
    "4. Experiment 1: visualize the training loss and feature activation frequency.\n",
    "5. Experiment 1: use a simple example to extract tokens that activate a specific feature.\n",
    "6. Experiment 2: use real transformer-generated MLP activations as input data.\n",
    "7. Experiment 2: use a pre-trained autoencoder for feature extraction and analysis.\n",
    "8. Experiment 2: compare feature activation distributions with those from experiment 1 and original paper.\n",
    "9. Experiment 2: conduct additional tests inspired by the paper to validate feature sparse monosemanticity.\n",
    "10. Compare our qualitative observations to the paper and document simplifications.\n",
    "\n",
    "**Simplifications Made:**  \n",
    "- Experiment 1: synthetic (random) data is used instead of actual Transformer activations.\n",
    "- Experiment 1: a single-layer autoencoder is employed.\n",
    "- Experiment 1: the training is performed for a limited number of epochs on a small dataset.\n",
    "- Experiment 2: while using real activations, the model size and dataset are still smaller than those in the paper.\n",
    "- Experiment 2: the pre-trained autoencoder is used as-is without additional fine-tuning.\n",
    "- Experiment 2: certain aspects of feature monosemanticity validation were simplified compared to the original paper but still provide strong supporting evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NMAUAEqkccD"
   },
   "source": [
    "## Here We Step into Experiment1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kRaLeHcwmy3"
   },
   "source": [
    "### Step1. Configuration and Initialization\n",
    "\n",
    "We set up our hyperparameters and global variables. These include the random seed, batch sizes, MLP dimensionality, expansion factor, learning rate, and others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformer_lens\n",
    "! pip install plotly gradio datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision -y\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pprint\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi\n",
    "from IPython.display import HTML, display\n",
    "from functools import partial\n",
    "import tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import colorsys\n",
    "import gradio as gr\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration dictionary\n",
    "cfg = {\n",
    "    \"seed\": 49,\n",
    "    \"batch_size\": 4096,\n",
    "    \"buffer_mult\": 384,\n",
    "    \"lr\": 1e-4,\n",
    "    \"num_tokens\": int(2e9),\n",
    "    \"l1_coeff\": 8e-4,\n",
    "    \"beta1\": 0.9,\n",
    "    \"beta2\": 0.99,\n",
    "    # Expansion factor: final features = d_mlp * dict_mult\n",
    "    \"dict_mult\": 8,\n",
    "    \"seq_len\": 128,\n",
    "    # Dimension of MLP activations\n",
    "    \"d_mlp\": 2048,\n",
    "    \"enc_dtype\": \"fp32\",\n",
    "    \"remove_rare_dir\": False,\n",
    "}\n",
    "cfg[\"model_batch_size\"] = 64\n",
    "cfg[\"buffer_size\"] = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
    "cfg[\"buffer_batches\"] = cfg[\"buffer_size\"] // cfg[\"seq_len\"]\n",
    "\n",
    "# Define data types\n",
    "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(cfg[\"seed\"])\n",
    "np.random.seed(cfg[\"seed\"])\n",
    "\n",
    "# Global variable for ablation\n",
    "neuron_to_ablate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmK0MeTEyvGK"
   },
   "source": [
    "### Step2. Sparse Autoencoder Definition\n",
    "\n",
    "We implement the sparse autoencoder that decomposes an input activation vector x from a Transformer MLP layer as: $x \\approx b + \\sum_i f_i(x) \\, d_i$\n",
    "\n",
    "\n",
    "where:\n",
    "-  $f(x) = \\mathrm{ReLU}(W_e (x - b_d) + b_e)$  is the encoder output (sparse activations), and\n",
    "- The decoder reconstructs $hat{x} = b + W_d \\, f(x)$.\n",
    "\n",
    "The loss is a combination of L2 reconstruction loss and L1 regularization to encourage sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Overcomplete dimension\n",
    "        d_hidden = cfg[\"d_mlp\"] * cfg[\"dict_mult\"]\n",
    "        d_mlp = cfg[\"d_mlp\"]\n",
    "        l1_coeff = cfg[\"l1_coeff\"]\n",
    "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
    "        torch.manual_seed(cfg[\"seed\"])\n",
    "\n",
    "        # Encoder: maps d_mlp to d_hidden\n",
    "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_mlp, d_hidden, dtype=dtype)))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "\n",
    "        # Decoder: maps d_hidden to d_mlp\n",
    "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, d_mlp, dtype=dtype)))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_mlp, dtype=dtype))\n",
    "\n",
    "        # Normalize each row of W_dec so that each feature direction is unit length\n",
    "        self.W_dec.data[:] = self.W_dec / (self.W_dec.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.l1_coeff = l1_coeff\n",
    "\n",
    "        self.to(\"cuda\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Center the input by subtracting decoder bias\n",
    "        x_cent = x - self.b_dec\n",
    "        # Encoder: Compute sparse activations with ReLU\n",
    "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
    "        # Decoder: Reconstruct the input from sparse codes\n",
    "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
    "        # L2 loss: reconstruction error\n",
    "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
    "        # L1 loss: sparsity regularization on activations\n",
    "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
    "        loss = l2_loss + l1_loss\n",
    "        return loss, x_reconstruct, acts, l2_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptCNVway3Rij"
   },
   "source": [
    "### Step3. Constructing a Simple Example\n",
    "\n",
    "- For demonstration, we simulate Transformer MLP activations with synthetic data.  \n",
    "- This synthetic data is a batch of random vectors with the same dimension as the MLP layer (2048).  \n",
    "- In a full replication, we would use real activations from a pretrained Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic data to simulate MLP activations\n",
    "batch_size = 256\n",
    "synthetic_data = torch.randn(batch_size, cfg[\"d_mlp\"]).cuda() # Here we use Chatgpt-4o to generate this code at 15:05pm 2/15/2025\n",
    "\n",
    "# Initialize the autoencoder\n",
    "encoder = AutoEncoder(cfg=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2krOi5gj3iBS"
   },
   "source": [
    "### Step4. Train the Autoencoder\n",
    "\n",
    "- We train the autoencoder on the synthetic data for a few epochs and record the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=cfg[\"lr\"])\n",
    "n_epochs = 100\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss, recon, acts, l2_loss, l1_loss = encoder(synthetic_data)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.item())\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: Total Loss={loss.item():.4f}, L2={l2_loss.item():.4f}, L1={l1_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dj8WFuK32e7"
   },
   "source": [
    "### Step5. Visualization: Training Loss Curve\n",
    "\n",
    "- We plot the training loss curve to verify that the autoencoder is learning over time.\n",
    "- Here we can see the autoencoder training loss decreases over epochs, indicating that the model learns to reconstruct synthetic MLP activations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(loss_history, label=\"Total Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oy2Aw0HF4Ez9"
   },
   "source": [
    "### Step6. Analyze Feature Activation Frequencies\n",
    "\n",
    "- One key observation in the paper is that many learned features are sparsely activated.  \n",
    "- Here, we compute the average activation frequency (i.e. the fraction of times each feature > 0) across multiple batches of synthetic data and plot a histogram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_freq(autoencoder, data, batch_size=32):\n",
    "    n_samples = data.size(0)\n",
    "    n_batches = n_samples // batch_size\n",
    "    freq_sum = torch.zeros(autoencoder.d_hidden).cuda()\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        batch = data[i*batch_size:(i+1)*batch_size]\n",
    "        with torch.no_grad():\n",
    "            _, _, acts, _, _ = encoder(batch)\n",
    "        freq_sum += (acts > 0).sum(dim=0).float() # Here we use Chatgpt-4o to redine this code at 15:30pm 2/15/2025\n",
    "    freq_avg = freq_sum / (n_batches * batch_size)\n",
    "    return freq_avg\n",
    "\n",
    "feature_freqs = compute_feature_freq(encoder, synthetic_data, batch_size=32)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(feature_freqs.cpu().numpy(), bins=50, color='skyblue', edgecolor='black')\n",
    "plt.xlabel(\"Activation Frequency\")\n",
    "plt.ylabel(\"Number of Features\")\n",
    "plt.title(\"Histogram of Feature Activation Frequencies on Semantic Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot log frequency\n",
    "log_freq = (feature_freqs).log10()\n",
    "px.histogram(utils.to_numpy(log_freq), title=\"Log Frequency of Features\", histnorm='percent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjWHwXTiFyjJ"
   },
   "source": [
    "1. Here we can see a distribution centered around 0.2-0.4, which means that many features are rarely activated, supporting the finding of sparse and monosemantic features in the paper.\n",
    "2. In the paper, the authors often observe a long tail of very low-frequency features, because the autoencoder is learning on real MLP activations from a large Transformer and is trained more times.\n",
    "3. However, this result can be reasonable for a toy and synthetic scenario with the chosen hyperparameters. With our toy data or fewer training steps, we may not replicate that exact shape like a big spike near zero. Our result is just a sign that our setup is less sparse than the paper’s final dictionary.\n",
    "4. If our goal is to replicate the high sparsity, we might consider increasing the L1 coefficient, training longer, or using real Transformer activations.\n",
    "5. I will replicate the true results in the 2nd section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBnftZYs8HT0"
   },
   "source": [
    "### Step7: Example: Extract Tokens Activating a Specific Feature\n",
    "\n",
    "- The original code includes helper functions (such as get_acts and make_token_df) to extract tokens that strongly activate a given feature from Transformer activations.  \n",
    "Here, we demonstrate this on synthetic data. (In a full replication, we would extract tokens from real transformer outputs.)\n",
    "\n",
    "- Below, get_acts filters examples where a specified feature is activated (activation > 0) and make_token_df creates a DataFrame showing tokens and their contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_acts(feature_id):\n",
    "    filtered_tokens = []\n",
    "    filtered_activations = []\n",
    "    # Loop over a number of batches\n",
    "    for _ in tqdm.trange(3):\n",
    "        # Randomly select a batch of tokens\n",
    "        # For synthetic example, we treat rows of synthetic_data as \"tokens\"\n",
    "        tokens = torch.randint(0, 10000, (cfg[\"model_batch_size\"], cfg[\"seq_len\"])).cuda()\n",
    "        # Simulate a \"cache\" by using synthetic data as if they were MLP activations(in practice, these activations come from a Transformer layer)\n",
    "        mlp_acts = torch.randn(cfg[\"model_batch_size\"], cfg[\"seq_len\"], cfg[\"d_mlp\"]).cuda()\n",
    "        mlp_acts_flattened = mlp_acts.reshape(-1, cfg[\"d_mlp\"])\n",
    "        loss, x_reconstruct, hidden_acts, l2_loss, l1_loss = encoder(mlp_acts_flattened)\n",
    "        b, c = tokens.shape\n",
    "        hidden = hidden_acts.reshape(b, c, -1)\n",
    "        # Extract the specified feature activations\n",
    "        feature_activations = hidden[..., feature_id]\n",
    "        # Only keep samples where the feature is active somewhere\n",
    "        batch_has_active_feature = (feature_activations > 0).any(dim=1).cpu()\n",
    "        if not batch_has_active_feature.any():\n",
    "            continue\n",
    "        filtered_tokens.append(tokens[batch_has_active_feature])\n",
    "        filtered_activations.append(feature_activations[batch_has_active_feature])\n",
    "        cnt = sum(len(i) for i in filtered_tokens)\n",
    "        if cnt > 200:\n",
    "            break\n",
    "    return torch.cat(filtered_tokens, dim=0), torch.cat(filtered_activations, dim=0)\n",
    "\n",
    "# For demonstration, choose feature index 10\n",
    "feature_id = 10\n",
    "filtered_tokens, filtered_activations = get_acts(feature_id)\n",
    "print(\"Filtered activations shape:\", filtered_activations.shape)\n",
    "print(\"Filtered tokens shape:\", filtered_tokens.shape)\n",
    "\n",
    "# Assume we have a helper function to convert token IDs to strings\n",
    "def fake_token_to_string(token_id):\n",
    "    return f\"token{token_id.item()}\"\n",
    "\n",
    "def make_token_df(tokens, len_prefix=3, len_suffix=3):\n",
    "    # Convert tokens to strings using a fake conversion function\n",
    "    str_tokens = [[fake_token_to_string(t) for t in sample] for sample in tokens.cpu().numpy()]\n",
    "    context = []\n",
    "    labels = []\n",
    "    for b in range(len(str_tokens)):\n",
    "        for p in range(len(str_tokens[b])):\n",
    "            prefix = \" \".join(str_tokens[b][max(0, p-len_prefix):p])\n",
    "            suffix = \" \".join(str_tokens[b][p+1:min(len(str_tokens[b]), p+1+len_suffix)])\n",
    "            current = str_tokens[b][p]\n",
    "            context.append(f\"{prefix} $ {current} $ {suffix}\")\n",
    "            labels.append(f\"{b}/{p}\") # Here we use ChatGpt-4o to refine this code at 16:00pm 2/15/2025\n",
    "    return pd.DataFrame({\"Token\": sum(str_tokens, []), \"Context\": context, \"Label\": labels})\n",
    "\n",
    "token_df = make_token_df(filtered_tokens)\n",
    "# Add the corresponding activation values\n",
    "token_df[\"Activation\"] = filtered_activations.view(-1).cpu().numpy()\n",
    "# Sort by activation descending and display the top 10\n",
    "display(token_df.sort_values(\"Activation\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2abjD7TmDSgA"
   },
   "source": [
    "* Note that here we use fake_token_to_string function, this is because (1) this helps avoid additional complexity and dependencies (2) in this demonstration, the data are randomly generated. Even if we used a real tokenizer, the output text would not have any meaningful semantic content. (3) the key objective is to show the entire pipeline, from obtaining transformer activations, processing them through the autoencoder, and finally associating these activations with tokens in a DataFrame. The exact text content is not that important.\n",
    "* Here we can see the activation values range from 1.62 to 2.00, indicating that certain tokens strongly activate by specific learned features. Tokens like token9427, token7001... have the highest activation values, meaning they are strongly associated with specific learned features (f10). This suggests that the sparse autoencoder is successfully identifying distinct feature activations and the certain tokens contribute more strongly to specific neuron responses.\n",
    "* The variation in activation values across different tokens suggests that the autoencoder has learned to differentiate between tokens.\n",
    "* The context column shows that each token appears in different textual environments, meaning that activations are influenced by surrounding tokens, which supports the idea that even in a sparse feature space, the token activations are still dependent on context.\n",
    "* Some tokens appear in different contexts but have similar activation strengths, which could mean that a single feature is still representing multiple meanings. However, this is expected as synthetic data lacks real-world semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHTk2nYrFUP5"
   },
   "source": [
    "### Step8. Conclusion and Discussion for Experiment1\n",
    "\n",
    "**Observations:**  \n",
    "1. The autoencoder training loss decreases over epochs, indicating that the model learns to reconstruct synthetic MLP activations.\n",
    "2. The histogram of feature activation frequencies shows that many features are rarely activated, supporting the finding of sparse features in the paper.\n",
    "3. Our example using get_acts and make_token_df demonstrates a method to extract tokens that activate a specific feature ( here we use feature 10 ).\n",
    "\n",
    "**Comparison with the Paper:**  \n",
    "Although we used synthetic data and a simplified model, the loss reduction, sparse activation, and interpretable feature extraction are consistent with the findings in the paper.\n",
    "\n",
    "**The Simplifications Made:**  \n",
    "1. Synthetic data is used instead of actual Transformer activations.\n",
    "2. We implemented a single-layer autoencoder and trained it for only 100 epochs on a small dataset.\n",
    "3. The token extraction example uses fake token IDs and a dummy conversion function.\n",
    "\n",
    "**Conclusion:**  \n",
    "1. This notebook so far demonstrates that a sparse autoencoder can effectively decompose activations into many features through a simplified implementation and example.\n",
    "2. In the next experiment, we will use real Transformer-generated MLP activations instead of synthetic data. This will allow us to evaluate whether the monosemanticity observed in experiment 1 holds when applied to actual neural network representations. Additionally, we will introduce a pre-trained autoencoder to improve feature extraction and conduct further tests to validate the presence of distinct, interpretable features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCk99Q3ArHXf"
   },
   "source": [
    "## Now We Move into Experiment2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoUlFJ2wrmQJ"
   },
   "source": [
    "### Step1. Configuration and Initialization\n",
    "* Compare with experiment1, here we add remove_parallel_component_of_grads and load_from_hf function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an AutoEncoder model class for sparse feature learning.\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # Overcomplete dimension\n",
    "        d_hidden = cfg[\"d_mlp\"] * cfg[\"dict_mult\"]\n",
    "        d_mlp = cfg[\"d_mlp\"]\n",
    "        l1_coeff = cfg[\"l1_coeff\"]\n",
    "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
    "        torch.manual_seed(cfg[\"seed\"])\n",
    "\n",
    "        # Encoder: maps d_mlp to d_hidden\n",
    "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_mlp, d_hidden, dtype=dtype)))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
    "\n",
    "        # Decoder: maps d_hidden to d_mlp\n",
    "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, d_mlp, dtype=dtype)))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_mlp, dtype=dtype))\n",
    "\n",
    "        # Normalize each row of W_dec so that each feature direction is unit length\n",
    "        self.W_dec.data[:] = self.W_dec / (self.W_dec.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "\n",
    "        self.d_hidden = d_hidden\n",
    "        self.l1_coeff = l1_coeff\n",
    "\n",
    "        self.to(\"cuda\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Center the input by subtracting decoder bias\n",
    "        x_cent = x - self.b_dec\n",
    "        # Encoder: Compute sparse activations with ReLU\n",
    "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
    "        # Decoder: Reconstruct the input from sparse codes\n",
    "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
    "        # L2 loss: reconstruction error\n",
    "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
    "        # L1 loss: sparsity regularization on activations\n",
    "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
    "        loss = l2_loss + l1_loss\n",
    "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def remove_parallel_component_of_grads(self):\n",
    "        \"\"\"\n",
    "        Removes gradient components that are parallel to decoder weights.\n",
    "        This helps maintain orthogonality in the learned features.\n",
    "        \"\"\"\n",
    "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
    "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
    "        self.W_dec.grad -= W_dec_grad_proj\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_hf(cls, version):\n",
    "        \"\"\"\n",
    "        Load a pre-trained autoencoder model from hugging face hub.\n",
    "        \"\"\"\n",
    "        if version == \"run1\":\n",
    "            version = 25\n",
    "        elif version == \"run2\":\n",
    "            version = 47\n",
    "\n",
    "        cfg = utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}_cfg.json\")\n",
    "        pprint.pprint(cfg)\n",
    "        self = cls(cfg=cfg)\n",
    "        self.load_state_dict(utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}.pt\", force_is_torch=True))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OmqSSVwtHGg"
   },
   "source": [
    "### Step2: Loading Pre-trained Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained 1-layer GELU transformer model from HookedTransformer library.\n",
    "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(DTYPES[cfg[\"enc_dtype\"]])\n",
    "\n",
    "# Extract model parameters.\n",
    "n_layers = model.cfg.n_layers\n",
    "d_model = model.cfg.d_model\n",
    "n_heads = model.cfg.n_heads\n",
    "d_head = model.cfg.d_head\n",
    "d_mlp = model.cfg.d_mlp\n",
    "d_vocab = model.cfg.d_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj8WCnIctNm_"
   },
   "source": [
    "### Step3: Preparing Dataset\n",
    "* Load dataset C4-Code-20K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
    "# Tokenize and concatenate text\n",
    "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "# Shuffle dataset with a fixed seed\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "# Extract token sequences from processed dataset\n",
    "all_tokens = tokenized_data[\"tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Py_8qJttaU4"
   },
   "source": [
    "### Step4: Loading Pre-trained Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which pre-trained autoencoder run to use (Here are two options: \"run1\" and \"run2\").\n",
    "auto_encoder_run = \"run1\" # @param [\"run1\", \"run2\"]\n",
    "encoder = AutoEncoder.load_from_hf(auto_encoder_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yukaMbntfhL"
   },
   "source": [
    "### Step5: Evaluating MLP Feature Reconstruction\n",
    "* Compute original model loss and compare it with reconstruction loss (MLP activations replaced by autoencoder outputs) and zero-ablation loss (MLP activations set to zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a hook function that replaces MLP activations with reconstructed activations.\n",
    "def replacement_hook(mlp_post, hook, encoder):\n",
    "    mlp_post_reconstr = encoder(mlp_post)[1]\n",
    "    return mlp_post_reconstr\n",
    "\n",
    "# Define a hook function that replaces MLP activations with their mean values.\n",
    "def mean_ablate_hook(mlp_post, hook):\n",
    "    mlp_post[:] = mlp_post.mean([0, 1])\n",
    "    return mlp_post\n",
    "\n",
    "# Define a hook function that zeros out MLP activations.\n",
    "def zero_ablate_hook(mlp_post, hook):\n",
    "    mlp_post[:] = 0.\n",
    "    return mlp_post\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_recons_loss(num_batches=5, local_encoder=None):\n",
    "    \"\"\"\n",
    "    Compute reconstruction loss and compare it with baseline and ablated losses.\n",
    "\n",
    "    Args:\n",
    "        num_batches: number of batches for evaluation.\n",
    "        local_encoder: the encoder used for reconstruction.\n",
    "\n",
    "    Returns:\n",
    "        score: reconstruction score.\n",
    "        loss: original model loss.\n",
    "        recons_loss: loss after reconstruction with autoencoder.\n",
    "        zero_abl_loss: loss when MLP activations are zeroed out.\n",
    "    \"\"\"\n",
    "    if local_encoder is None:\n",
    "        local_encoder = encoder\n",
    "    loss_list = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        # Select a random batch of tokens\n",
    "        tokens = all_tokens[torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]]\n",
    "\n",
    "        # Compute original model loss\n",
    "        loss = model(tokens, return_type=\"loss\")\n",
    "\n",
    "        # Compute loss after replacing MLP activations with reconstructed ones\n",
    "        recons_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), partial(replacement_hook, encoder=local_encoder))])\n",
    "\n",
    "        # Compute loss after zeroing out MLP activations\n",
    "        zero_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), zero_ablate_hook)])\n",
    "\n",
    "        loss_list.append((loss, recons_loss, zero_abl_loss))\n",
    "\n",
    "    # Compute mean losses across batches\n",
    "    losses = torch.tensor(loss_list)\n",
    "    loss, recons_loss, zero_abl_loss = losses.mean(0).tolist()\n",
    "\n",
    "    # Print losses\n",
    "    print(f\"loss: {loss:.4f}, recons_loss: {recons_loss:.4f}, zero_abl_loss: {zero_abl_loss:.4f}\")\n",
    "\n",
    "    # Compute reconstruction score\n",
    "    score = ((zero_abl_loss - recons_loss) / (zero_abl_loss - loss))\n",
    "    print(f\"Reconstruction Score: {score:.2%}\")\n",
    "\n",
    "    return score, loss, recons_loss, zero_abl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the reconstruction loss using the pre-trained autoencoder, which evaluates how well the autoencoder can reconstruct MLP activations.\n",
    "_ = get_recons_loss(num_batches=20, local_encoder=encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPXFXDbx-d0Q"
   },
   "source": [
    "* We can see that the reconstruction loss is close to the original loss. This suggests that a small set of extracted features can retain most of the original model’s behavior, which supports the paper's hypothesis that MLP activations can be decomposed into a meaningful feature space.\n",
    "* A 91.12% reconstruction score means that the autoencoder’s extracted sparse features can explain most of the variance in the original activations. This implies that MLP activations are not fully distributed but instead dominated by a small number of key features (neurons), suggesting that some neurons might each correspond to an interpretable concept.\n",
    "* Here the reconstruction loss being slightly higher than the original loss (3.7475 vs. 3.2575) suggests that some amount of superposition still exists, but the autoencoder is capable of reducing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4tBu5kuuN8B"
   },
   "source": [
    "### Step6. Activation Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute frequency of neuron activations using an autoencoder.\n",
    "@torch.no_grad()\n",
    "def get_freqs(num_batches=25, local_encoder=None):\n",
    "    \"\"\"\n",
    "    Analyze the frequency of neuron activations.\n",
    "\n",
    "    Args:\n",
    "        num_batches: number of batches to sample.\n",
    "        local_encoder: the encoder used for feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        act_freq_scores: frequency of each neuron's activation.\n",
    "    \"\"\"\n",
    "    if local_encoder is None:\n",
    "        local_encoder = encoder\n",
    "\n",
    "    # Initialize activation frequency storage (on GPU)\n",
    "    act_freq_scores = torch.zeros(local_encoder.d_hidden, dtype=torch.float32).cuda()\n",
    "    total = 0\n",
    "\n",
    "    for i in tqdm.trange(num_batches):\n",
    "        # Select a random batch of tokens\n",
    "        tokens = all_tokens[torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]]\n",
    "\n",
    "        # Run model and extract cached activations from the first MLP layer\n",
    "        _, cache = model.run_with_cache(tokens, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
    "        mlp_acts = cache[utils.get_act_name(\"post\", 0)]\n",
    "        # Flatten activations\n",
    "        mlp_acts = mlp_acts.reshape(-1, d_mlp)\n",
    "\n",
    "        # Encode activations to get sparse features\n",
    "        hidden = local_encoder(mlp_acts)[2]\n",
    "\n",
    "        # Count active neurons (where activation > 0)\n",
    "        act_freq_scores += (hidden > 0).sum(0)\n",
    "        total += hidden.shape[0]\n",
    "\n",
    "    # Normalize activation frequencies\n",
    "    act_freq_scores /= total\n",
    "\n",
    "    # Compute percentage of \"dead\" neurons (never activated)\n",
    "    num_dead = (act_freq_scores == 0).float().mean()\n",
    "    print(\"Num dead\", num_dead)\n",
    "\n",
    "    return act_freq_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute activation frequencies for each neuron in the MLP layer.\n",
    "freqs = get_freqs(num_batches=200, local_encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 1e-6.5 so that dead features show up as log_freq -6.5\n",
    "log_freq = (freqs + 10**-6.5).log10()\n",
    "px.histogram(utils.to_numpy(log_freq), title=\"Log Frequency of Features\", histnorm='percent') # Here we use Chatgpt4o to refine this code at 17:35pm 2/15/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77M7HIL4FXem"
   },
   "source": [
    "* We can see the majority of features in the histogram are concentrated on the left (around -4.5 or lower), meaning that most neurons rarely activate. This supports the paper's hypothesis that the MLP layer is using a sparse feature representation, where only a small subset of neurons activate frequently, while most remain inactive.\n",
    "* Also, we see a long tail extending towards higher log_freq values. This suggests that a few neurons are activated much more often than the rest, which are likely responsible for encoding critical, monosemantic features， which are likely for those undamental, commonly used features. This aligns with the paper’s claim that Transformer MLP layers contain a few \"monosemantic neurons\" to respond to specific concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uj1iLEo3u4vn"
   },
   "source": [
    "### Step7: Select Features to Analyze Their Activation Frequency Percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific feature indices to analyze their activation frequency percentiles\n",
    "indices = [44, 100, 2126, 9686]\n",
    "\n",
    "# Compute the activation frequency percentile for each selected feature\n",
    "percentile_data = []\n",
    "for index in indices:\n",
    "    # Get the activation frequency of the feature\n",
    "    value = freqs[index].item()\n",
    "    # Count how many features have lower frequency\n",
    "    less_than = torch.lt(freqs, value).sum().item()\n",
    "    # Compute percentile ranking\n",
    "    percentile = 100.0 * less_than / len(freqs)\n",
    "    percentile_data.append((index, \"{:.1e}\".format(value), percentile))\n",
    "\n",
    "# Convert data into a DataFrame for easy visualization\n",
    "df = pd.DataFrame(percentile_data, columns=['Feature', 'Frequency', 'Percentile'])\n",
    "\n",
    "# Sort the DataFrame by activation frequency\n",
    "df_sorted = df.sort_values(by='Frequency')\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCXNnvwHJVK3"
   },
   "source": [
    "* If the percentile is low, this suggests that the neuron is not activated frequently across all inputs. Instead, it is likely highly specialized, meaning it only activates in specific conditions. This behavior matches the monosemantic neurons hypothesis, where certain neurons respond to only one or a few specific features.\n",
    "* If the percentile is high, which means this neuron is activated more frequently. It may still encode meaningful information but is less specialized than the low-percentile neurons.\n",
    "* So, here we choose those low-frequency features to see whether they are highly specialized or simply inactive/dead neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jw-6L4Y7vcZ5"
   },
   "source": [
    "### Step8: Select Specific feature to Analyze Their Behavior in Transformer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import escape\n",
    "import colorsys\n",
    "from IPython.display import display\n",
    "\n",
    "# Symbol for space visualization\n",
    "SPACE = \"\\u00b7\"\n",
    "# Symbol for new line visualization\n",
    "NEWLINE = \"\\u21b5\"\n",
    "# Symbol for new line visualization\n",
    "TAB = \"\\u2192\"\n",
    "\n",
    "def create_html(strings, values, max_value=None, saturation=0.5, allow_different_length=False, return_string=False):\n",
    "    \"\"\"\n",
    "    Generates an HTML visualization of token activations.\n",
    "\n",
    "    Args:\n",
    "      strings: list of tokens to be visualized.\n",
    "      values: activation values corresponding to each token.\n",
    "      max_value: maximum absolute value for scaling colors.\n",
    "      saturation: intensity of color saturation.\n",
    "      allow_different_length: whether to allow different lengths for strings and values.\n",
    "\n",
    "    Returns:\n",
    "      html: html string representing the visualization.\n",
    "    \"\"\"\n",
    "    escaped_strings = [escape(s, quote=True) for s in strings]\n",
    "    processed_strings = [\n",
    "        s.replace(\"\\n\", f\"{NEWLINE}<br/>\").replace(\"\\t\", f\"{TAB}&emsp;\").replace(\" \", \"&nbsp;\")\n",
    "        for s in escaped_strings\n",
    "    ]\n",
    "\n",
    "    if isinstance(values, torch.Tensor) and len(values.shape) > 1:\n",
    "        values = values.flatten().tolist()\n",
    "\n",
    "    if not allow_different_length:\n",
    "        assert len(processed_strings) == len(values)\n",
    "\n",
    "    if max_value is None:\n",
    "        # Prevent division by zero\n",
    "        max_value = max(max(values), -min(values)) + 1e-3\n",
    "    scaled_values = [v / max_value * saturation for v in values]\n",
    "\n",
    "    html = \"\"\n",
    "    for i, s in enumerate(processed_strings):\n",
    "        v = scaled_values[i] if i < len(scaled_values) else 0\n",
    "        # Red for negative values, Blue for positive values\n",
    "        hue = 0 if v < 0 else 0.66\n",
    "        # Convert HSV to RGB\n",
    "        rgb_color = colorsys.hsv_to_rgb(hue, v, 1)\n",
    "        hex_color = \"#%02x%02x%02x\" % (\n",
    "            int(rgb_color[0] * 255), int(rgb_color[1] * 255), int(rgb_color[2] * 255)\n",
    "        )\n",
    "        html += f'<span style=\"background-color: {hex_color}; border: 1px solid lightgray; font-size: 16px; border-radius: 3px;\">{s}</span>'\n",
    "\n",
    "    if return_string:\n",
    "        return html\n",
    "    else:\n",
    "        display(HTML(html)) # This function is refined by Chatgpt4o at 17:50pm 2/15/2025\n",
    "\n",
    "def basic_token_vis_make_str(strings, values, max_val=None):\n",
    "    \"\"\"\n",
    "    Creates an HTML-based token visualization with color-coded activations.\n",
    "    \"\"\"\n",
    "    if not isinstance(strings, list):\n",
    "        strings = model.to_str_tokens(strings)\n",
    "    values = utils.to_numpy(values)\n",
    "    if max_val is None:\n",
    "        max_val = values.max()\n",
    "\n",
    "    header_string = f\"<h4>Max Range <b>{values.max():.4f}</b> Min Range: <b>{values.min():.4f}</b></h4>\"\n",
    "    header_string += f\"<h4>Set Max Range <b>{max_val:.4f}</b></h4>\"\n",
    "\n",
    "    body_string = create_html(strings, values, max_value=max_val, return_string=True)\n",
    "    return header_string + body_string # This function is refined by Chatgpt4o at 18:10pm 2/15/2025\n",
    "\n",
    "def basic_feature_vis(text, feature_index, max_val=0):\n",
    "    \"\"\"\n",
    "    Computes and visualizes feature activations for a given input text.\n",
    "\n",
    "    Args:\n",
    "     text: the input text.\n",
    "     feature_index: the neuron feature index to analyze.\n",
    "     max_val: maximum activation value for color scaling.\n",
    "    \"\"\"\n",
    "    feature_in = encoder.W_enc[:, feature_index]\n",
    "    feature_bias = encoder.b_enc[feature_index]\n",
    "    _, cache = model.run_with_cache(text, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
    "    mlp_acts = cache[utils.get_act_name(\"post\", 0)][0]\n",
    "    feature_acts = torch.relu((mlp_acts - encoder.b_dec) @ feature_in + feature_bias)\n",
    "\n",
    "    if max_val == 0:\n",
    "        max_val = max(1e-7, feature_acts.max().item())\n",
    "\n",
    "    return basic_token_vis_make_str(text, feature_acts, max_val) # This function is refined by Chatgpt4o at 18:25pm 2/15/2025\n",
    "\n",
    "def process_token(s):\n",
    "    \"\"\"\n",
    "    Process individual tokens to replace whitespace, newlines, and tabs with visual symbols.\n",
    "    \"\"\"\n",
    "    if isinstance(s, torch.Tensor) or isinstance(s, np.int64) or isinstance(s, int):\n",
    "        s = model.to_string(s.item())\n",
    "    s = s.replace(\" \", SPACE)\n",
    "    s = s.replace(\"\\n\", NEWLINE + \"\\n\")\n",
    "    s = s.replace(\"\\t\", TAB)\n",
    "    return s\n",
    "\n",
    "def process_tokens(l):\n",
    "    \"\"\"\n",
    "    Process a list of tokens into readable format.\n",
    "    \"\"\"\n",
    "    if isinstance(l, str):\n",
    "        l = model.to_str_tokens(l)\n",
    "    elif isinstance(l, torch.Tensor) and len(l.shape) > 1:\n",
    "        l = l.squeeze(0)\n",
    "    return [process_token(s) for s in l]\n",
    "\n",
    "def list_flatten(nested_list):\n",
    "    \"\"\"\n",
    "    Flatten a nested list.\n",
    "    \"\"\"\n",
    "    return [x for y in nested_list for x in y]\n",
    "\n",
    "def make_feature_vis_gradio(feature_id, starting_text=None, batch=None, pos=None):\n",
    "    \"\"\"\n",
    "    Creates an interactive Gradio interface for visualizing neuron activations.\n",
    "    \"\"\"\n",
    "    if starting_text is None:\n",
    "        starting_text = model.to_string(all_tokens[batch, 1:pos+1])\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.HTML(value=f\"Hacky Interactive Neuroscope for gelu-1l\")\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                text = gr.Textbox(label=\"Text\", value=starting_text)\n",
    "                feature_index = gr.Number(label=\"Feature Index\", value=feature_id, precision=0)\n",
    "                max_val = gr.Number(label=\"Max Value\", value=None)\n",
    "                inputs = [text, feature_index, max_val]\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                out = gr.HTML(label=\"Neuron Acts\", value=basic_feature_vis(starting_text, feature_id))\n",
    "        for inp in inputs:\n",
    "            inp.change(basic_feature_vis, inputs, out)\n",
    "    demo.launch(share=True) # This function is refined by Chatgpt4o at 18:45pm 2/15/2025\n",
    "\n",
    "def make_token_df(tokens, len_prefix=10, len_suffix=10):\n",
    "    \"\"\"\n",
    "    Constructs a DataFrame containing token sequences and their context.\n",
    "\n",
    "    Args:\n",
    "      tokens: tokenized input sequences.\n",
    "      len_prefix: number of context tokens before the target token.\n",
    "      len_suffix: mumber of context tokens after the target token.\n",
    "    \"\"\"\n",
    "    str_tokens = [process_tokens(model.to_str_tokens(t)) for t in tokens]\n",
    "\n",
    "    context = []\n",
    "    label = []\n",
    "    for b in range(tokens.shape[0]):\n",
    "        for p in range(tokens.shape[1]):\n",
    "            prefix = \"\".join(str_tokens[b][max(0, p-len_prefix):p])\n",
    "            suffix = \"\".join(str_tokens[b][p+1:min(tokens.shape[1]-1, p+1+len_suffix)])\n",
    "            current = str_tokens[b][p]\n",
    "            context.append(f\"{prefix}${current}${suffix}\")\n",
    "            label.append(f\"{b}/{p}\")\n",
    "\n",
    "    return pd.DataFrame(dict(\n",
    "        str_tokens=list_flatten(str_tokens),\n",
    "        context=context,\n",
    "        label=label,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific feature indices\n",
    "features = [15, 100, 2126, 9686]\n",
    "\n",
    "# Define the feature ID for analysis\n",
    "# This neuron will be analyzed in detail\n",
    "feature_id = 9686  # @param {type:\"number\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"number\"}\n",
    "\n",
    "# Print activation frequency of the selected neuron\n",
    "print(f\"Feature freq: {freqs[feature_id].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_acts(feature_id):\n",
    "    \"\"\"\n",
    "    Extract activations of a given neuron across multiple input sequences.\n",
    "\n",
    "    Args:\n",
    "        feature_id: neuron index to analyze.\n",
    "\n",
    "    Returns:\n",
    "        Token sequences where the neuron was activated.\n",
    "        Activation values for these sequences.\n",
    "    \"\"\"\n",
    "    filtered_tokens = []\n",
    "    filtered_activations = []\n",
    "\n",
    "    # Iterate over 200 random batches\n",
    "    for _ in tqdm.trange(200):\n",
    "        tokens = all_tokens[torch.randperm(len(all_tokens))[:batch_size]]\n",
    "        _, cache = model.run_with_cache(tokens, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
    "        # Extract MLP activations\n",
    "        mlp_acts = cache[utils.get_act_name(\"post\", 0)]\n",
    "\n",
    "        del cache, _\n",
    "\n",
    "        # Get batch and sequence length\n",
    "        b, c = tokens.shape\n",
    "        mlp_acts_flattened = mlp_acts.reshape(-1, cfg[\"d_mlp\"])\n",
    "\n",
    "        # Pass activations through the autoencoder to extract sparse features\n",
    "        loss, x_reconstruct, hidden_acts, l2_loss, l1_loss = encoder(mlp_acts_flattened)\n",
    "\n",
    "        # Reshape back into batch and sequence format\n",
    "        hidden = hidden_acts.reshape(b, c, -1)\n",
    "        del loss, x_reconstruct, hidden_acts, l2_loss, l1_loss\n",
    "\n",
    "        # Extract the activation values of the specific feature\n",
    "        feature_activations = hidden[..., feature_id]\n",
    "\n",
    "        # Find token sequences where this feature was activated\n",
    "        batch_has_active_feature = (feature_activations > 0).any(dim=1).cpu()\n",
    "        if not batch_has_active_feature.any():\n",
    "            continue\n",
    "\n",
    "        # Store token sequences and activations\n",
    "        filtered_tokens.append(tokens[batch_has_active_feature])\n",
    "        filtered_activations.append(feature_activations[batch_has_active_feature])\n",
    "\n",
    "        # Stop collecting when at least 200 sequences are found\n",
    "        cnt = (sum([len(i) for i in filtered_tokens]))\n",
    "        if cnt > 200:\n",
    "            break\n",
    "\n",
    "    return torch.cat(filtered_tokens, dim=0), torch.cat(filtered_activations, dim=0)\n",
    "\n",
    "# Extract token sequences where the feature is strongly activated\n",
    "filtered_tokens, filtered_activations = get_acts(feature_id)\n",
    "\n",
    "# Print the shapes of the extracted activations and token sequences\n",
    "print(\"hidden_acts.shape\", filtered_activations.shape, filtered_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = make_token_df(filtered_tokens)\n",
    "token_df[\"feature\"] = utils.to_numpy(filtered_activations.view(-1))\n",
    "token_df.sort_values(\"feature\", ascending=False).head(15).style.background_gradient(\"coolwarm\") # This function is refined by Chatgpt4o at 19:25pm 2/15/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVr5oZyOMrwP"
   },
   "source": [
    "* We can see the highest activations appear for tokens like \"·Er\" (common in German) and \"·Ger\" (short for Germany). This suggests that this feature might be encoding a linguistic pattern related to German words or names.\n",
    "* And we can also see this patern from context column. Tokens like \"Er\" and \"Ger\" appear in similar linguistic contexts with German structure. This suggests that even though this feature is low-frequency, it is still active in specific cases.\n",
    "* The activations for some tokens drop significantly but do not reach zero. This could mean that the feature is only relevant in specific cases, and it could also indicate some overlap with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_text = \"A different potential constitutional barrier was identified by Joseph Gerth, who argued in his column for the Louisville\" # @param {type:\"string\"}\n",
    "make_feature_vis_gradio(feature_id, starting_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGFOdgKJUIx6"
   },
   "source": [
    "## Key Findings:\n",
    "\n",
    "1. Experiment 1: sparse autoencoder successfully decomposes Transformer MLP activations into a sparse feature space, confirming the possibility for disentangling features.\n",
    "\n",
    "2. Experiment 1: most extracted features are rarely activated, supporting the idea that Transformer activations are sparsely distributed rather than fully polysemantic.\n",
    "\n",
    "3. Experiment 1: feature activation analysis shows that certain features respond selectively to specific tokens, indicating early signs of monosemanticity.\n",
    "\n",
    "4. Experiment 2: sparse decomposition holds for real Transformer activations, validating that monosemantic neurons exist beyond synthetic data.\n",
    "\n",
    "5. Experiment 2: High reconstruction score (91.12%) shows that a small number of extracted features can preserve most of the model’s predictive capacity.\n",
    "\n",
    "6. Experiment 2: Low-frequency features are highly specialized, activating in specific contexts rather than being entirely inactive.\n",
    "\n",
    "7. Experiment 2: Some extracted features correspond to specific linguistic patterns like German words, suggesting that Transformer neurons encode interpretable concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8-s0YIBUl-5"
   },
   "source": [
    "## Brief Writeup for the Notebook\n",
    "1. What I Learned About the Mechanism:\n",
    "- Transformer MLP activations can be effectively decomposed using a sparse autoencoder, confirming that these activations are not fully distributed.\n",
    "- The sparse autoencoder successfully extracts meaningful, independent features, reinforcing the idea that monosemantic neurons exist.\n",
    "- Certain neurons activate only in very specific contexts, suggesting that they encode highly specialized information.\n",
    "\n",
    "2. Challenges Encountered:\n",
    "- The issue betweem synthetic data and real data. Training the autoencoder on synthetic data showed promising results, but real Transformer activations introduced additional complexities, which may require a more complex pre-trained model.\n",
    "- There still exists feature overlap. Some extracted features showed partial overlap, meaning that the sparse decomposition is not perfect and still retains some degree of superposition.\n",
    "- Identifying whether low-frequency features are truly monosemantic or just noise required additional experiments.\n",
    "\n",
    "3. Potential Extensions and Improvements:\n",
    "- We need to train autoencoder on larger datasets. By this way, we could lead to better feature extraction and improved interpretability.\n",
    "- We need to further fine tune autoencoder for feature orthogonality. For example, adding additional constraints or regularization might improve feature separation and reduce overlap.\n",
    "- We also need to investigating higher-order monosemanticity. Instead of focusing on single-layer MLP activations, applying this approach to deeper Transformer layers might reveal more abstract monosemantic representations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
