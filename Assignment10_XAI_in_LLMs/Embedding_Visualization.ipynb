{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ZzzTheGamer/XAI/blob/Assignment10_XAI_in_LLMs/Embedding_Visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWAeEy_dBvLb"
   },
   "source": [
    "## Embedding_Visualization\n",
    "* External Resources：\n",
    "- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
    "- [Sentence Transformers Documentation](https://www.sbert.net/)\n",
    "- [NLTK Documentation](https://www.nltk.org/)\n",
    "- [UMAP Documentation](https://umap-learn.readthedocs.io/)\n",
    "- [t-SNE Overview](https://lvdmaaten.github.io/tsne/)\n",
    "- Gpt4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bca0f13"
   },
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required third-party libraries\n",
    "!pip install sentence-transformers adjustText nltk umap-learn ipywidgets plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "173dcd63"
   },
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import plotly.express as px\n",
    "from ipywidgets import interact, IntSlider, FloatSlider\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d666e971"
   },
   "source": [
    "### Download NLTK Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Download required datasets for NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words as nltk_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa57adbd"
   },
   "source": [
    "### Load Pretrained Embedding Model from MTEB Leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained embedding model from MTEB leaderboard\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b68e4784"
   },
   "source": [
    "### Load and Encode Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk.words dataset (~236k English words), selecting the first 2000 as a subset\n",
    "subset_size = 2000\n",
    "all_words = list(set(nltk_words.words()))\n",
    "words_list = all_words[:subset_size]\n",
    "\n",
    "# Compute embeddings for each word\n",
    "embeddings = model.encode(words_list, show_progress_bar=True)\n",
    "\n",
    "# Perform POS tagging using NLTK\n",
    "pos_tags = nltk.pos_tag(words_list)\n",
    "pos_labels = [tag for _, tag in pos_tags]\n",
    "\n",
    "# Assign different colors to POS tags for visualization\n",
    "unique_pos = list(set(pos_labels))\n",
    "color_map = plt.cm.get_cmap('tab20', len(unique_pos))\n",
    "pos_to_color = {pos: color_map(i) for i, pos in enumerate(unique_pos)}\n",
    "colors = [pos_to_color[pos] for pos in pos_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7cba323"
   },
   "source": [
    "### Visualize 2D Embeddings using PCA, t-SNE, and UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot layout: 2 rows x 4 columns\n",
    "fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n",
    "\n",
    "# PCA Baseline (First plot in first row)\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "axes[0, 0].scatter(embeddings_pca[:, 0], embeddings_pca[:, 1], c=colors, s=10)\n",
    "axes[0, 0].set_title('PCA Baseline')\n",
    "\n",
    "# t-SNE Parameter Comparison: Different perplexity values in first-row plots\n",
    "perplexities = [15, 30, 50]\n",
    "for i, perplexity in enumerate(perplexities):\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    embeddings_tsne = tsne.fit_transform(embeddings)\n",
    "    axes[0, i + 1].scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], c=colors, s=10)\n",
    "    axes[0, i + 1].set_title(f't-SNE (perplexity={perplexity})')\n",
    "\n",
    "# UMAP Parameter Comparison: Different n_neighbors values in second-row plots\n",
    "n_neighbors_options = [5, 15, 30]\n",
    "for i, n_neighbors in enumerate(n_neighbors_options):\n",
    "    umap_model = umap.UMAP(n_components=2, n_neighbors=n_neighbors, min_dist=0.1, random_state=42)\n",
    "    embeddings_umap = umap_model.fit_transform(embeddings)\n",
    "    axes[1, i].scatter(embeddings_umap[:, 0], embeddings_umap[:, 1], c=colors, s=10)\n",
    "    axes[1, i].set_title(f'UMAP (n_neighbors={n_neighbors})')\n",
    "\n",
    "\n",
    "axes[1, 3].axis('off')\n",
    "# Add POS legend\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                      markerfacecolor=pos_to_color[pos], markersize=10)\n",
    "           for pos in unique_pos]\n",
    "fig.legend(handles, unique_pos, loc='lower right', ncol=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGldztxrz69W"
   },
   "source": [
    "* From this graph, we can see there’s no clear separation of colors in PCA model. This is because language embeddings often have complex and nonlinear relationships, a simple linear projection tends to produce one main cluster.\n",
    "* For the nonlineart focus model, SNE tries to preserve local distances so that points that are similar remain close together. Higher perplexity (e.g. 50) can spread points out more uniformly and capture somewhat broader relationships.However, we still see no strong grouping in this model. This is normal if the model’s embeddings don’t primarily cluster words by grammatical function.\n",
    "* For the model UMAP, UMAP balances local and global structure well than t-SNE. Larger n_neighbors (30) looks more globally, sometimes merging smaller clusters into a broader shape. Again, the color distribution suggests the scatter distribution and no dominant clustering purely by POS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8d9b8b4"
   },
   "source": [
    "### 3D Visualization using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from ipywidgets import interact\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 3D PCA Visualization (Fixed Parameters)\n",
    "def visualize_3d_pca():\n",
    "    reducer = PCA(n_components=3, random_state=42)\n",
    "    embeddings_3d = reducer.fit_transform(embeddings)\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        x=embeddings_3d[:, 0],\n",
    "        y=embeddings_3d[:, 1],\n",
    "        z=embeddings_3d[:, 2],\n",
    "        color=pos_labels,\n",
    "        hover_name=words_list,\n",
    "        title=\"3D PCA Visualization with POS Tags\",\n",
    "        labels={'color': 'Part of Speech'}\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    fig.show()\n",
    "\n",
    "visualize_3d_pca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmwiIuD22_Vh"
   },
   "source": [
    "* Same as the previous conclusion, the colors are mostly mixed for PCA model. This indicates that the first three principal components do not strongly separate words by grammatical function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a33bb2bc"
   },
   "source": [
    "### 3D Visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# 3D t-SNE Visualization (Fixed Parameters)\n",
    "def visualize_3d_tsne(perplexity=30):\n",
    "    reducer = TSNE(n_components=3, random_state=42, perplexity=perplexity)\n",
    "    embeddings_3d = reducer.fit_transform(embeddings)\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        x=embeddings_3d[:, 0],\n",
    "        y=embeddings_3d[:, 1],\n",
    "        z=embeddings_3d[:, 2],\n",
    "        color=pos_labels,\n",
    "        hover_name=words_list,\n",
    "        title=f\"3D t-SNE Visualization with POS Tags (perplexity={perplexity})\",\n",
    "        labels={'color': 'Part of Speech'}\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    fig.show()\n",
    "\n",
    "visualize_3d_tsne()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyyH41Fg3P6t"
   },
   "source": [
    "* Same as the previous conclusion, we may see small pockets of words, but overall it still forms a single blob. T-SNE tends to sacrifice some global big picture arrangement, so while local clusters may appear, the overall distribution can look scattered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81ef59bf"
   },
   "source": [
    "### 3D Visualization using UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "# 3D UMAP Visualization (Fixed Parameters)\n",
    "def visualize_3d_umap():\n",
    "    reducer = umap.UMAP(n_components=3, random_state=42)\n",
    "    embeddings_3d = reducer.fit_transform(embeddings)\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        x=embeddings_3d[:, 0],\n",
    "        y=embeddings_3d[:, 1],\n",
    "        z=embeddings_3d[:, 2],\n",
    "        color=pos_labels,\n",
    "        hover_name=words_list,\n",
    "        title=\"3D UMAP Visualization with POS Tags\",\n",
    "        labels={'color': 'Part of Speech'}\n",
    "    )\n",
    "    fig.update_traces(marker=dict(size=3))\n",
    "    fig.show()\n",
    "\n",
    "visualize_3d_umap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvGm9Alq3rXU"
   },
   "source": [
    "* Same as the previous conclusion, as with t-SNE, POS tags don’t drive strong clustering in the embedding space. This is because (1) embeddings capture semantics not grammatical function (2) Some single words lack context like run\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4f884d0"
   },
   "source": [
    "### Therefore, here, We've Done Semantic Similarity Analysis using Cosine Similarity & UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def most_similar_words(target_word, n=5):\n",
    "    \"\"\"\n",
    "    For a given word target_word, compute the n similar words with the highest cosine similarity\n",
    "    \"\"\"\n",
    "    if target_word not in words_list:\n",
    "        print(f\"Word '{target_word}' is not in the data set.\")\n",
    "        return []\n",
    "    idx = words_list.index(target_word)\n",
    "    target_embedding = embeddings[idx]\n",
    "    # Compute cosine similarity between all words and the target word\n",
    "    similarities = np.dot(embeddings, target_embedding) / (norm(embeddings, axis=1) * norm(target_embedding) + 1e-10)\n",
    "    # Retrieve the top n+1 most similar words (including itself)\n",
    "    top_indices = np.argsort(similarities)[-n-1:][::-1]\n",
    "    similar = [words_list[i] for i in top_indices if words_list[i] != target_word][:n]\n",
    "    return similar\n",
    "\n",
    "def plot_similar_words(word, n=5):\n",
    "    \"\"\"\n",
    "    Draw the semantic relationship diagram of target words and their similar words\n",
    "    \"\"\"\n",
    "    similar_words = most_similar_words(word, n)\n",
    "    if not similar_words:\n",
    "        return\n",
    "    all_words_sim = [word] + similar_words\n",
    "    # Calculate the embedding of these words\n",
    "    target_embeddings = model.encode(all_words_sim)\n",
    "    # Use UMAP to reduce high-dimensional vectors to two dimensions\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    embeddings_2d = reducer.fit_transform(target_embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c='lightgrey')\n",
    "    texts = []\n",
    "    for i, (x, y) in enumerate(embeddings_2d):\n",
    "        plt.plot([embeddings_2d[0, 0], x], [embeddings_2d[0, 1], y], 'grey', alpha=0.3)\n",
    "        texts.append(plt.text(x, y, all_words_sim[i], fontsize=12)) #using chatgpt4o to refine this function at 1:40pm 3/22/2025\n",
    "    adjust_text(texts)\n",
    "    plt.title(f\"Semantic Relationships: '{word}' and Similar Words\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eg: View the semantic relationship of \"heterogeneous\"\n",
    "plot_similar_words(\"heterogeneous\", n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FgDm9Ye5sNv"
   },
   "source": [
    "* From this graph, we can see heterogeneous is at the center.\n",
    "\n",
    "* Words like “inhomogeneous,” “heterize,” “heteroscopy,” “homoeomery,” are surrounding words. We can see the embedding model often captures not just pure semantics, but also morphological patterns (“hetero-” “homo-”). Many of these share prefixes and thus appear similar in the model’s embedding space.\n",
    "\n",
    "* The lines show direct connections from “heterogeneous” to each neighbor, visually emphasizing that these are the closest matches in high-dimensional space.“inhomogeneous” is nearly a synonym for “heterogeneous,” so it makes sense it’s among the closest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dedc944b"
   },
   "source": [
    "### Clustering Evaluation using KMeans & Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_clustering(embeddings_2d, n_clusters=15):\n",
    "    \"\"\"\n",
    "    Perform KMeans clustering on the reduced data and calculate the Silhouette score\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(embeddings_2d)\n",
    "    score = silhouette_score(embeddings_2d, labels)\n",
    "    return score\n",
    "\n",
    "# Compute clustering quality for PCA, t-SNE, and UMAP reduced data\n",
    "methods = {\n",
    "    'PCA': PCA(n_components=2, random_state=42).fit_transform(embeddings),\n",
    "    't-SNE': TSNE(n_components=2, random_state=42).fit_transform(embeddings),\n",
    "    'UMAP': umap.UMAP(n_components=2, random_state=42).fit_transform(embeddings)\n",
    "}\n",
    "\n",
    "scores = {}\n",
    "for name, emb in methods.items():\n",
    "    scores[name] = evaluate_clustering(emb, n_clusters=15)\n",
    "\n",
    "print(\"Clustering quality（Silhouette Score）：\")\n",
    "for name, score in scores.items():\n",
    "    print(f\"{name}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO8bOCSE7o_7"
   },
   "source": [
    "* The overall silhouette score around 0.3 indicates the clusters are not very distinct. This may reflect that 15 clusters could be too many or too few, or that the embeddings themselves do not naturally form strongly separated groups in 2D.\n",
    "* A higher Silhouette score indicates better separation between clusters. This is represented by points within the same cluster are more similar, and points in different clusters are more dissimilar. UMAP’s higher score shows that its 2D layout makes it easier for KMeans to form clearer clusters than PCA or t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpzdIn9d9b2o"
   },
   "source": [
    "## Comparison betweem PCA, t-SNE, UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9gaMIbx9o0M"
   },
   "source": [
    "* PCA is a linear method that projects data onto directions of maximum variance.\n",
    "1. Strengths: Fast, preserves overall global structure, and is simple to compute.\n",
    "2. Weaknesses: May compress complex, nonlinear relationships into one large blob; limited in presenting fine local structure.\n",
    "\n",
    "* t-SNE is a nonlinear method focusing on preserving local neighborhoods.\n",
    "1. Strengths: Excellent at uncovering local clusters and revealing fine grouping.\n",
    "2. Weaknesses: Loses global structure; results can be sensitive to parameter choices and may vary between runs.\n",
    "\n",
    "* UMAP ia a nonlinear manifold learning technique that balances local and global structure.\n",
    "1. Strengths: Faster than t-SNE, scales well with large datasets, and produces a 2D layout that reflects both local and broader patterns.\n",
    "2. Weaknesses: Results can still vary with parameter settings, and fine-tuning may be necessary."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
