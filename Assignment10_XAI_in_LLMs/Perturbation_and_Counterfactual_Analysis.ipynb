{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ZzzTheGamer/XAI/blob/Assignment10_XAI_in_LLMs/Perturbation_and_Counterfactual_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qL3LYg-Mi4h_"
   },
   "source": [
    "# XAI in LLMs: Perturbation and Counterfactual Analysis\n",
    "* Referencces:\n",
    "* Hugging Face Transformers Library\n",
    "* Sentence Transformers (all-MiniLM-L6-v2)\n",
    "* XAI in NLP: https://arxiv.org/abs/2012.10807"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaSdAK5bjG5i"
   },
   "source": [
    "##  Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers sentence-transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GenerationConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLTqlt49jMr8"
   },
   "source": [
    "## Load Language and Similarity Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "similarity_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJH7CvxijRpo"
   },
   "source": [
    "## Define Response Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        generation_config=GenerationConfig(\n",
    "            max_new_tokens=50,      # Limit the length of the generated text\n",
    "            do_sample=False,       # Disable sampling\n",
    "            temperature=0.0,       # No randomness in generation\n",
    "            top_k=1           # Only consider the top token for generation\n",
    "        )\n",
    "    )\n",
    "    # Decode the generated token ids into a human-readable string\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tw0kUcEkI_J"
   },
   "source": [
    "## Base Prompt Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"The knight fought the dragon with a sword. The knight won because\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H4DRB2fkO2q"
   },
   "source": [
    "## Define Perturbations and Counterfactual Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations = {\n",
    "    'symbols': [\n",
    "        \"The wizard fought the dragon with a spell. The wizard won because\",\n",
    "        \"The knight fought the demon with a sword. The knight won because\"\n",
    "    ],\n",
    "    'patterns': [\n",
    "        \"With a sword, the dragon was fought by the knight. The knight won because\",\n",
    "        \"The dragon was fought by the knight, who used a sword. The knight won because\"\n",
    "    ],\n",
    "    'text': [\n",
    "        \"In a futuristic city, a robot battled a drone using lasers. The robot won because\",\n",
    "        \"A chef competed against a rival in a cooking contest. The chef won because\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactuals = [\n",
    "    \"The knight fought the dragon with a sword. The knight lost because\",\n",
    "    \"The knight fought the dragon with a sword. The knight barely survived because\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soxf86EkkYd7"
   },
   "source": [
    "## Generate Responses and Compute Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_response = generate_response(base_prompt)\n",
    "base_embedding = similarity_model.encode([base_response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store similarity scores for each perturbation type\n",
    "results = {}\n",
    "\n",
    "# Compute similarities for each perturbed prompt\n",
    "for key, prompts in perturbations.items():\n",
    "    similarities = []\n",
    "    for prompt in prompts:\n",
    "        response = generate_response(prompt)\n",
    "        embedding = similarity_model.encode([response])\n",
    "        similarity = np.dot(base_embedding, embedding.T).item()\n",
    "        similarities.append(similarity)\n",
    "    results[key] = np.mean(similarities)\n",
    "\n",
    "# Compute similarities for counterfactual prompts\n",
    "cf_similarities = []\n",
    "for prompt in counterfactuals:\n",
    "    response = generate_response(prompt)\n",
    "    embedding = similarity_model.encode([response])\n",
    "    similarity = np.dot(base_embedding, embedding.T).item()\n",
    "    cf_similarities.append(similarity)\n",
    "\n",
    "results['counterfactual'] = np.mean(cf_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii-Igf9wkqlj"
   },
   "source": [
    "## Visualization of Perturbation Impact on Output Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(results.keys(), results.values())\n",
    "plt.title('Impact of Perturbations on Output Similarity')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "plt.xlabel('Perturbation Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOTJTdHCkvT9"
   },
   "source": [
    "## Print Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results Summary:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUsWI1Ndk0YF"
   },
   "source": [
    "## View Prompts and Generated Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View base response\n",
    "print(\"Base Prompt and Response\")\n",
    "print(\"Prompt:\", base_prompt)\n",
    "print(\"Response:\", base_response)\n",
    "print(\"\\n-----------------------------\\n\")\n",
    "\n",
    "# View responses for each perturbation type\n",
    "for key, prompts in perturbations.items():\n",
    "    print(f\"Perturbation Type: {key}\")\n",
    "    for prompt in prompts:\n",
    "        response = generate_response(prompt)\n",
    "        print(\"Prompt:\", prompt)\n",
    "        print(\"Response:\", response)\n",
    "        print(\"-----------------------------\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# View counterfactual responses\n",
    "print(\"Counterfactual Prompts and Responses\")\n",
    "for prompt in counterfactuals:\n",
    "    response = generate_response(prompt)\n",
    "    print(\"Prompt:\", prompt)\n",
    "    print(\"Response:\", response)\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OW6LopkRm9_8"
   },
   "source": [
    "## Results Analysis Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rElxrJdjnAPI"
   },
   "source": [
    "* Text Perturbation (Cosine Similarity = 0.30)\n",
    "：This category shows the lowest similarity score, indicating that changing the entire context has the greatest impact on the generated output. For example, replacing a medieval scenario like \"the knight fought the dragon\" with a futuristic battle between a robot and a drone or a cooking contest involving a chef leads to a complete transformation in scene, characters, and narrative. As a result, the semantic similarity between the base response and the perturbed outputs is significantly reduced.\n",
    "\n",
    "* Symbols Perturbation (Cosine Similarity = 0.68)\n",
    ": Altering key symbols, such as replacing \"knight\" with \"wizard\" or \"dragon\" with \"demon,\" produces a moderate impact on the output. While the specific roles are changed, the core narrative of a battle and victory remains intact. Therefore, although there are noticeable differences, the outputs retain a relatively high degree of similarity to the base response.\n",
    "\n",
    "* Patterns Perturbation (Cosine Similarity = 0.9)\n",
    ": This category focuses on structural modifications, such as shifting from active to passive voice. The main content, who fights whom with what and who wins, remains unchanged. As a result, the generated outputs are semantically very close to the original, leading to a high similarity score. This suggests that syntactic variations have minimal influence on the model’s semantic generation.\n",
    "\n",
    "* Counterfactual Perturbation (Cosine Similarity = 0.9)\n",
    ": Counterfactual prompts introduce minimal changes aimed at altering the outcome, for example, changing \"the knight won\" to \"the knight lost\" or \"barely survived.\" Despite these changes in the conclusion, the rest of the prompt, including the setting, characters, and actions, stays nearly identical. The generated outputs reflect this maintaining a high level of similarity with the base response. The key difference is typically confined to a small portion of the text, resulting in the highest similarity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APunL1thpHe8"
   },
   "source": [
    "# Potential Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-tNGP1OpMVN"
   },
   "source": [
    "1. Model Selection: Consider experimenting with more advanced models (e.g.GPT-3 or open-source LLaMA/OPT models). Larger models can provide more varied outputs, which may result in more nuanced similarity scores.\n",
    "\n",
    "2. Diversity of Perturbations: Introduce automated or algorithmic perturbation generation to ensure more comprehensive coverage. For example, experiment with fine-grained changes, such as modifying adjectives, adverbs, or negations.\n",
    "\n",
    "3. Similarity Metrics: Exploring other evaluation metrics (BLEU, ROUGE, etc.) may provide additional perspectives to evaluate performance.\n",
    "\n",
    "4. Various Explainability Techniques: Incorporating saliency scores or attention analysis could further interpret model behavior."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
